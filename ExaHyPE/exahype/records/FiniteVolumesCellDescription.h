#ifndef _EXAHYPE_RECORDS_FINITEVOLUMESCELLDESCRIPTION_H
#define _EXAHYPE_RECORDS_FINITEVOLUMESCELLDESCRIPTION_H

#include "peano/utils/Globals.h"
#include "tarch/compiler/CompilerSpecificSettings.h"
#include "peano/utils/PeanoOptimisations.h"
#ifdef Parallel
	#include "tarch/parallel/Node.h"
#endif
#ifdef Parallel
	#include <mpi.h>
#endif
#include "tarch/logging/Log.h"
#include "tarch/la/Vector.h"
#include <bitset>
#include <complex>
#include <string>
#include <iostream>

namespace exahype {
   namespace records {
      class FiniteVolumesCellDescription;
      class FiniteVolumesCellDescriptionPacked;
   }
}

#if defined(Parallel)
   /**
    * @author This class is generated by DaStGen
    * 		   DataStructureGenerator (DaStGen)
    * 		   2007-2009 Wolfgang Eckhardt
    * 		   2012      Tobias Weinzierl
    *
    * 		   build date: 09-02-2014 14:40
    *
    * @date   26/09/2017 12:31
    */
   class exahype::records::FiniteVolumesCellDescription { 
      
      public:
         
         typedef exahype::records::FiniteVolumesCellDescriptionPacked Packed;
         
         enum CompressionState {
            Uncompressed = 0, CurrentlyProcessed = 1, Compressed = 2
         };
         
         enum RefinementEvent {
            None = 0, ErasingChildrenRequested = 1, ErasingChildren = 2, ChangeChildrenToDescendantsRequested = 3, ChangeChildrenToDescendants = 4, RefiningRequested = 5, Refining = 6, DeaugmentingChildrenRequestedTriggered = 7, DeaugmentingChildrenRequested = 8, DeaugmentingChildren = 9, AugmentingRequested = 10, Augmenting = 11
         };
         
         enum Type {
            Erased = 0, Ancestor = 1, Cell = 2, Descendant = 3
         };
         
         struct PersistentRecords {
            int _solverNumber;
            double _timeStepSize;
            double _timeStamp;
            double _previousTimeStepSize;
            int _solution;
            int _solutionAverages;
            int _solutionCompressed;
            int _previousSolution;
            int _previousSolutionAverages;
            int _previousSolutionCompressed;
            int _extrapolatedSolution;
            int _extrapolatedSolutionAverages;
            int _extrapolatedSolutionCompressed;
            CompressionState _compressionState;
            int _bytesPerDoFInPreviousSolution;
            int _bytesPerDoFInSolution;
            int _bytesPerDoFInExtrapolatedSolution;
            int _level;
            #ifdef UseManualAlignment
            tarch::la::Vector<DIMENSIONS,double> _offset __attribute__((aligned(VectorisationAlignment)));
            #else
            tarch::la::Vector<DIMENSIONS,double> _offset;
            #endif
            #ifdef UseManualAlignment
            tarch::la::Vector<DIMENSIONS,double> _size __attribute__((aligned(VectorisationAlignment)));
            #else
            tarch::la::Vector<DIMENSIONS,double> _size;
            #endif
            #ifdef UseManualAlignment
            std::bitset<DIMENSIONS_TIMES_TWO> _neighbourMergePerformed __attribute__((aligned(VectorisationAlignment)));
            #else
            std::bitset<DIMENSIONS_TIMES_TWO> _neighbourMergePerformed;
            #endif
            #ifdef UseManualAlignment
            std::bitset<DIMENSIONS_TIMES_TWO> _isInside __attribute__((aligned(VectorisationAlignment)));
            #else
            std::bitset<DIMENSIONS_TIMES_TWO> _isInside;
            #endif
            bool _oneRemoteBoundaryNeighbourIsOfTypeCell;
            #ifdef UseManualAlignment
            tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> _faceDataExchangeCounter __attribute__((aligned(VectorisationAlignment)));
            #else
            tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> _faceDataExchangeCounter;
            #endif
            Type _type;
            int _parentIndex;
            RefinementEvent _refinementEvent;
            /**
             * Generated
             */
            PersistentRecords();
            
            /**
             * Generated
             */
            PersistentRecords(const int& solverNumber, const double& timeStepSize, const double& timeStamp, const double& previousTimeStepSize, const int& solution, const int& solutionAverages, const int& solutionCompressed, const int& previousSolution, const int& previousSolutionAverages, const int& previousSolutionCompressed, const int& extrapolatedSolution, const int& extrapolatedSolutionAverages, const int& extrapolatedSolutionCompressed, const CompressionState& compressionState, const int& bytesPerDoFInPreviousSolution, const int& bytesPerDoFInSolution, const int& bytesPerDoFInExtrapolatedSolution, const int& level, const tarch::la::Vector<DIMENSIONS,double>& offset, const tarch::la::Vector<DIMENSIONS,double>& size, const std::bitset<DIMENSIONS_TIMES_TWO>& neighbourMergePerformed, const std::bitset<DIMENSIONS_TIMES_TWO>& isInside, const bool& oneRemoteBoundaryNeighbourIsOfTypeCell, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& faceDataExchangeCounter, const Type& type, const int& parentIndex, const RefinementEvent& refinementEvent);
            
            
            inline int getSolverNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _solverNumber;
            }
            
            
            
            inline void setSolverNumber(const int& solverNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _solverNumber = solverNumber;
            }
            
            
            
            inline double getTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _timeStepSize;
            }
            
            
            
            inline void setTimeStepSize(const double& timeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _timeStepSize = timeStepSize;
            }
            
            
            
            inline double getTimeStamp() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _timeStamp;
            }
            
            
            
            inline void setTimeStamp(const double& timeStamp) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _timeStamp = timeStamp;
            }
            
            
            
            inline double getPreviousTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _previousTimeStepSize;
            }
            
            
            
            inline void setPreviousTimeStepSize(const double& previousTimeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _previousTimeStepSize = previousTimeStepSize;
            }
            
            
            
            inline int getSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _solution;
            }
            
            
            
            inline void setSolution(const int& solution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _solution = solution;
            }
            
            
            
            inline int getSolutionAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _solutionAverages;
            }
            
            
            
            inline void setSolutionAverages(const int& solutionAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _solutionAverages = solutionAverages;
            }
            
            
            
            inline int getSolutionCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _solutionCompressed;
            }
            
            
            
            inline void setSolutionCompressed(const int& solutionCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _solutionCompressed = solutionCompressed;
            }
            
            
            
            inline int getPreviousSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _previousSolution;
            }
            
            
            
            inline void setPreviousSolution(const int& previousSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _previousSolution = previousSolution;
            }
            
            
            
            inline int getPreviousSolutionAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _previousSolutionAverages;
            }
            
            
            
            inline void setPreviousSolutionAverages(const int& previousSolutionAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _previousSolutionAverages = previousSolutionAverages;
            }
            
            
            
            inline int getPreviousSolutionCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _previousSolutionCompressed;
            }
            
            
            
            inline void setPreviousSolutionCompressed(const int& previousSolutionCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _previousSolutionCompressed = previousSolutionCompressed;
            }
            
            
            
            inline int getExtrapolatedSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _extrapolatedSolution;
            }
            
            
            
            inline void setExtrapolatedSolution(const int& extrapolatedSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _extrapolatedSolution = extrapolatedSolution;
            }
            
            
            
            inline int getExtrapolatedSolutionAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _extrapolatedSolutionAverages;
            }
            
            
            
            inline void setExtrapolatedSolutionAverages(const int& extrapolatedSolutionAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _extrapolatedSolutionAverages = extrapolatedSolutionAverages;
            }
            
            
            
            inline int getExtrapolatedSolutionCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _extrapolatedSolutionCompressed;
            }
            
            
            
            inline void setExtrapolatedSolutionCompressed(const int& extrapolatedSolutionCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _extrapolatedSolutionCompressed = extrapolatedSolutionCompressed;
            }
            
            
            
            inline CompressionState getCompressionState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _compressionState;
            }
            
            
            
            inline void setCompressionState(const CompressionState& compressionState) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _compressionState = compressionState;
            }
            
            
            
            inline int getBytesPerDoFInPreviousSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _bytesPerDoFInPreviousSolution;
            }
            
            
            
            inline void setBytesPerDoFInPreviousSolution(const int& bytesPerDoFInPreviousSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _bytesPerDoFInPreviousSolution = bytesPerDoFInPreviousSolution;
            }
            
            
            
            inline int getBytesPerDoFInSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _bytesPerDoFInSolution;
            }
            
            
            
            inline void setBytesPerDoFInSolution(const int& bytesPerDoFInSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _bytesPerDoFInSolution = bytesPerDoFInSolution;
            }
            
            
            
            inline int getBytesPerDoFInExtrapolatedSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _bytesPerDoFInExtrapolatedSolution;
            }
            
            
            
            inline void setBytesPerDoFInExtrapolatedSolution(const int& bytesPerDoFInExtrapolatedSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _bytesPerDoFInExtrapolatedSolution = bytesPerDoFInExtrapolatedSolution;
            }
            
            
            
            inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _level;
            }
            
            
            
            inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _level = level;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<DIMENSIONS,double> getOffset() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _offset;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setOffset(const tarch::la::Vector<DIMENSIONS,double>& offset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _offset = (offset);
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<DIMENSIONS,double> getSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _size;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setSize(const tarch::la::Vector<DIMENSIONS,double>& size) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _size = (size);
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline std::bitset<DIMENSIONS_TIMES_TWO> getNeighbourMergePerformed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _neighbourMergePerformed;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setNeighbourMergePerformed(const std::bitset<DIMENSIONS_TIMES_TWO>& neighbourMergePerformed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _neighbourMergePerformed = (neighbourMergePerformed);
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline std::bitset<DIMENSIONS_TIMES_TWO> getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _isInside;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setIsInside(const std::bitset<DIMENSIONS_TIMES_TWO>& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _isInside = (isInside);
            }
            
            
            
            inline bool getOneRemoteBoundaryNeighbourIsOfTypeCell() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _oneRemoteBoundaryNeighbourIsOfTypeCell;
            }
            
            
            
            inline void setOneRemoteBoundaryNeighbourIsOfTypeCell(const bool& oneRemoteBoundaryNeighbourIsOfTypeCell) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _oneRemoteBoundaryNeighbourIsOfTypeCell = oneRemoteBoundaryNeighbourIsOfTypeCell;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> getFaceDataExchangeCounter() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _faceDataExchangeCounter;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setFaceDataExchangeCounter(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& faceDataExchangeCounter) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _faceDataExchangeCounter = (faceDataExchangeCounter);
            }
            
            
            
            inline Type getType() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _type;
            }
            
            
            
            inline void setType(const Type& type) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _type = type;
            }
            
            
            
            inline int getParentIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _parentIndex;
            }
            
            
            
            inline void setParentIndex(const int& parentIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _parentIndex = parentIndex;
            }
            
            
            
            inline RefinementEvent getRefinementEvent() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _refinementEvent;
            }
            
            
            
            inline void setRefinementEvent(const RefinementEvent& refinementEvent) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _refinementEvent = refinementEvent;
            }
            
            
            
         };
         private: 
            PersistentRecords _persistentRecords;
            
         public:
            /**
             * Generated
             */
            FiniteVolumesCellDescription();
            
            /**
             * Generated
             */
            FiniteVolumesCellDescription(const PersistentRecords& persistentRecords);
            
            /**
             * Generated
             */
            FiniteVolumesCellDescription(const int& solverNumber, const double& timeStepSize, const double& timeStamp, const double& previousTimeStepSize, const int& solution, const int& solutionAverages, const int& solutionCompressed, const int& previousSolution, const int& previousSolutionAverages, const int& previousSolutionCompressed, const int& extrapolatedSolution, const int& extrapolatedSolutionAverages, const int& extrapolatedSolutionCompressed, const CompressionState& compressionState, const int& bytesPerDoFInPreviousSolution, const int& bytesPerDoFInSolution, const int& bytesPerDoFInExtrapolatedSolution, const int& level, const tarch::la::Vector<DIMENSIONS,double>& offset, const tarch::la::Vector<DIMENSIONS,double>& size, const std::bitset<DIMENSIONS_TIMES_TWO>& neighbourMergePerformed, const std::bitset<DIMENSIONS_TIMES_TWO>& isInside, const bool& oneRemoteBoundaryNeighbourIsOfTypeCell, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& faceDataExchangeCounter, const Type& type, const int& parentIndex, const RefinementEvent& refinementEvent);
            
            /**
             * Generated
             */
            ~FiniteVolumesCellDescription();
            
            
            inline int getSolverNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._solverNumber;
            }
            
            
            
            inline void setSolverNumber(const int& solverNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._solverNumber = solverNumber;
            }
            
            
            
            inline double getTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._timeStepSize;
            }
            
            
            
            inline void setTimeStepSize(const double& timeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._timeStepSize = timeStepSize;
            }
            
            
            
            inline double getTimeStamp() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._timeStamp;
            }
            
            
            
            inline void setTimeStamp(const double& timeStamp) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._timeStamp = timeStamp;
            }
            
            
            
            inline double getPreviousTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._previousTimeStepSize;
            }
            
            
            
            inline void setPreviousTimeStepSize(const double& previousTimeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._previousTimeStepSize = previousTimeStepSize;
            }
            
            
            
            inline int getSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._solution;
            }
            
            
            
            inline void setSolution(const int& solution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._solution = solution;
            }
            
            
            
            inline int getSolutionAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._solutionAverages;
            }
            
            
            
            inline void setSolutionAverages(const int& solutionAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._solutionAverages = solutionAverages;
            }
            
            
            
            inline int getSolutionCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._solutionCompressed;
            }
            
            
            
            inline void setSolutionCompressed(const int& solutionCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._solutionCompressed = solutionCompressed;
            }
            
            
            
            inline int getPreviousSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._previousSolution;
            }
            
            
            
            inline void setPreviousSolution(const int& previousSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._previousSolution = previousSolution;
            }
            
            
            
            inline int getPreviousSolutionAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._previousSolutionAverages;
            }
            
            
            
            inline void setPreviousSolutionAverages(const int& previousSolutionAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._previousSolutionAverages = previousSolutionAverages;
            }
            
            
            
            inline int getPreviousSolutionCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._previousSolutionCompressed;
            }
            
            
            
            inline void setPreviousSolutionCompressed(const int& previousSolutionCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._previousSolutionCompressed = previousSolutionCompressed;
            }
            
            
            
            inline int getExtrapolatedSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._extrapolatedSolution;
            }
            
            
            
            inline void setExtrapolatedSolution(const int& extrapolatedSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._extrapolatedSolution = extrapolatedSolution;
            }
            
            
            
            inline int getExtrapolatedSolutionAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._extrapolatedSolutionAverages;
            }
            
            
            
            inline void setExtrapolatedSolutionAverages(const int& extrapolatedSolutionAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._extrapolatedSolutionAverages = extrapolatedSolutionAverages;
            }
            
            
            
            inline int getExtrapolatedSolutionCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._extrapolatedSolutionCompressed;
            }
            
            
            
            inline void setExtrapolatedSolutionCompressed(const int& extrapolatedSolutionCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._extrapolatedSolutionCompressed = extrapolatedSolutionCompressed;
            }
            
            
            
            inline CompressionState getCompressionState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._compressionState;
            }
            
            
            
            inline void setCompressionState(const CompressionState& compressionState) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._compressionState = compressionState;
            }
            
            
            
            inline int getBytesPerDoFInPreviousSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._bytesPerDoFInPreviousSolution;
            }
            
            
            
            inline void setBytesPerDoFInPreviousSolution(const int& bytesPerDoFInPreviousSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._bytesPerDoFInPreviousSolution = bytesPerDoFInPreviousSolution;
            }
            
            
            
            inline int getBytesPerDoFInSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._bytesPerDoFInSolution;
            }
            
            
            
            inline void setBytesPerDoFInSolution(const int& bytesPerDoFInSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._bytesPerDoFInSolution = bytesPerDoFInSolution;
            }
            
            
            
            inline int getBytesPerDoFInExtrapolatedSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._bytesPerDoFInExtrapolatedSolution;
            }
            
            
            
            inline void setBytesPerDoFInExtrapolatedSolution(const int& bytesPerDoFInExtrapolatedSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._bytesPerDoFInExtrapolatedSolution = bytesPerDoFInExtrapolatedSolution;
            }
            
            
            
            inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._level;
            }
            
            
            
            inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._level = level;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<DIMENSIONS,double> getOffset() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._offset;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setOffset(const tarch::la::Vector<DIMENSIONS,double>& offset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._offset = (offset);
            }
            
            
            
            inline double getOffset(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS);
               return _persistentRecords._offset[elementIndex];
               
            }
            
            
            
            inline void setOffset(int elementIndex, const double& offset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS);
               _persistentRecords._offset[elementIndex]= offset;
               
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<DIMENSIONS,double> getSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._size;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setSize(const tarch::la::Vector<DIMENSIONS,double>& size) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._size = (size);
            }
            
            
            
            inline double getSize(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS);
               return _persistentRecords._size[elementIndex];
               
            }
            
            
            
            inline void setSize(int elementIndex, const double& size) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS);
               _persistentRecords._size[elementIndex]= size;
               
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline std::bitset<DIMENSIONS_TIMES_TWO> getNeighbourMergePerformed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._neighbourMergePerformed;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setNeighbourMergePerformed(const std::bitset<DIMENSIONS_TIMES_TWO>& neighbourMergePerformed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._neighbourMergePerformed = (neighbourMergePerformed);
            }
            
            
            
            inline bool getNeighbourMergePerformed(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS_TIMES_TWO);
               return _persistentRecords._neighbourMergePerformed[elementIndex];
               
            }
            
            
            
            inline void setNeighbourMergePerformed(int elementIndex, const bool& neighbourMergePerformed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS_TIMES_TWO);
               _persistentRecords._neighbourMergePerformed[elementIndex]= neighbourMergePerformed;
               
            }
            
            
            
            inline void flipNeighbourMergePerformed(int elementIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS_TIMES_TWO);
               _persistentRecords._neighbourMergePerformed.flip(elementIndex);
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline std::bitset<DIMENSIONS_TIMES_TWO> getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._isInside;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setIsInside(const std::bitset<DIMENSIONS_TIMES_TWO>& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._isInside = (isInside);
            }
            
            
            
            inline bool getIsInside(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS_TIMES_TWO);
               return _persistentRecords._isInside[elementIndex];
               
            }
            
            
            
            inline void setIsInside(int elementIndex, const bool& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS_TIMES_TWO);
               _persistentRecords._isInside[elementIndex]= isInside;
               
            }
            
            
            
            inline void flipIsInside(int elementIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS_TIMES_TWO);
               _persistentRecords._isInside.flip(elementIndex);
            }
            
            
            
            inline bool getOneRemoteBoundaryNeighbourIsOfTypeCell() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._oneRemoteBoundaryNeighbourIsOfTypeCell;
            }
            
            
            
            inline void setOneRemoteBoundaryNeighbourIsOfTypeCell(const bool& oneRemoteBoundaryNeighbourIsOfTypeCell) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._oneRemoteBoundaryNeighbourIsOfTypeCell = oneRemoteBoundaryNeighbourIsOfTypeCell;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> getFaceDataExchangeCounter() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._faceDataExchangeCounter;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setFaceDataExchangeCounter(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& faceDataExchangeCounter) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._faceDataExchangeCounter = (faceDataExchangeCounter);
            }
            
            
            
            inline int getFaceDataExchangeCounter(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS_TIMES_TWO);
               return _persistentRecords._faceDataExchangeCounter[elementIndex];
               
            }
            
            
            
            inline void setFaceDataExchangeCounter(int elementIndex, const int& faceDataExchangeCounter) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS_TIMES_TWO);
               _persistentRecords._faceDataExchangeCounter[elementIndex]= faceDataExchangeCounter;
               
            }
            
            
            
            inline Type getType() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._type;
            }
            
            
            
            inline void setType(const Type& type) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._type = type;
            }
            
            
            
            inline int getParentIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._parentIndex;
            }
            
            
            
            inline void setParentIndex(const int& parentIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._parentIndex = parentIndex;
            }
            
            
            
            inline RefinementEvent getRefinementEvent() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._refinementEvent;
            }
            
            
            
            inline void setRefinementEvent(const RefinementEvent& refinementEvent) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._refinementEvent = refinementEvent;
            }
            
            
            /**
             * Generated
             */
            static std::string toString(const CompressionState& param);
            
            /**
             * Generated
             */
            static std::string getCompressionStateMapping();
            
            /**
             * Generated
             */
            static std::string toString(const RefinementEvent& param);
            
            /**
             * Generated
             */
            static std::string getRefinementEventMapping();
            
            /**
             * Generated
             */
            static std::string toString(const Type& param);
            
            /**
             * Generated
             */
            static std::string getTypeMapping();
            
            /**
             * Generated
             */
            std::string toString() const;
            
            /**
             * Generated
             */
            void toString(std::ostream& out) const;
            
            
            PersistentRecords getPersistentRecords() const;
            /**
             * Generated
             */
            FiniteVolumesCellDescriptionPacked convert() const;
            
            
         #ifdef Parallel
            protected:
               static tarch::logging::Log _log;
               
            public:
               
               /**
                * Global that represents the mpi datatype.
                * There are two variants: Datatype identifies only those attributes marked with
                * parallelise. FullDatatype instead identifies the whole record with all fields.
                */
               static MPI_Datatype Datatype;
               static MPI_Datatype FullDatatype;
               
               /**
                * Initializes the data type for the mpi operations. Has to be called
                * before the very first send or receive operation is called.
                */
               static void initDatatype();
               
               static void shutdownDatatype();
               
               /**
                * @param communicateSleep -1 Data exchange through blocking mpi
                * @param communicateSleep  0 Data exchange through non-blocking mpi, i.e. pending messages are received via polling until MPI_Test succeeds
                * @param communicateSleep >0 Same as 0 but in addition, each unsuccessful MPI_Test is follows by an usleep
                */
               void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, int communicateSleep);
               
               void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, int communicateSleep);
               
               static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
               
               #endif
      
   };
   
   #ifndef DaStGenPackedPadding
     #define DaStGenPackedPadding 1      // 32 bit version
     // #define DaStGenPackedPadding 2   // 64 bit version
   #endif
   
   
   #ifdef PackedRecords
      #pragma pack (push, DaStGenPackedPadding)
   #endif
   
   /**
    * @author This class is generated by DaStGen
    * 		   DataStructureGenerator (DaStGen)
    * 		   2007-2009 Wolfgang Eckhardt
    * 		   2012      Tobias Weinzierl
    *
    * 		   build date: 09-02-2014 14:40
    *
    * @date   26/09/2017 12:31
    */
   class exahype::records::FiniteVolumesCellDescriptionPacked { 
      
      public:
         
         typedef exahype::records::FiniteVolumesCellDescription::CompressionState CompressionState;
         
         typedef exahype::records::FiniteVolumesCellDescription::Type Type;
         
         typedef exahype::records::FiniteVolumesCellDescription::RefinementEvent RefinementEvent;
         
         struct PersistentRecords {
            int _solverNumber;
            double _timeStepSize;
            double _timeStamp;
            double _previousTimeStepSize;
            int _solution;
            int _solutionAverages;
            int _solutionCompressed;
            int _previousSolution;
            int _previousSolutionAverages;
            int _previousSolutionCompressed;
            int _extrapolatedSolution;
            int _extrapolatedSolutionAverages;
            int _extrapolatedSolutionCompressed;
            int _level;
            tarch::la::Vector<DIMENSIONS,double> _offset;
            tarch::la::Vector<DIMENSIONS,double> _size;
            std::bitset<DIMENSIONS_TIMES_TWO> _neighbourMergePerformed;
            std::bitset<DIMENSIONS_TIMES_TWO> _isInside;
            bool _oneRemoteBoundaryNeighbourIsOfTypeCell;
            tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> _faceDataExchangeCounter;
            Type _type;
            int _parentIndex;
            RefinementEvent _refinementEvent;
            
            /** mapping of records:
            || Member 	|| startbit 	|| length
             |  compressionState	| startbit 0	| #bits 2
             |  bytesPerDoFInPreviousSolution	| startbit 2	| #bits 3
             |  bytesPerDoFInSolution	| startbit 5	| #bits 3
             |  bytesPerDoFInExtrapolatedSolution	| startbit 8	| #bits 3
             */
            int _packedRecords0;
            
            /**
             * Generated
             */
            PersistentRecords();
            
            /**
             * Generated
             */
            PersistentRecords(const int& solverNumber, const double& timeStepSize, const double& timeStamp, const double& previousTimeStepSize, const int& solution, const int& solutionAverages, const int& solutionCompressed, const int& previousSolution, const int& previousSolutionAverages, const int& previousSolutionCompressed, const int& extrapolatedSolution, const int& extrapolatedSolutionAverages, const int& extrapolatedSolutionCompressed, const CompressionState& compressionState, const int& bytesPerDoFInPreviousSolution, const int& bytesPerDoFInSolution, const int& bytesPerDoFInExtrapolatedSolution, const int& level, const tarch::la::Vector<DIMENSIONS,double>& offset, const tarch::la::Vector<DIMENSIONS,double>& size, const std::bitset<DIMENSIONS_TIMES_TWO>& neighbourMergePerformed, const std::bitset<DIMENSIONS_TIMES_TWO>& isInside, const bool& oneRemoteBoundaryNeighbourIsOfTypeCell, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& faceDataExchangeCounter, const Type& type, const int& parentIndex, const RefinementEvent& refinementEvent);
            
            
            inline int getSolverNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _solverNumber;
            }
            
            
            
            inline void setSolverNumber(const int& solverNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _solverNumber = solverNumber;
            }
            
            
            
            inline double getTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _timeStepSize;
            }
            
            
            
            inline void setTimeStepSize(const double& timeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _timeStepSize = timeStepSize;
            }
            
            
            
            inline double getTimeStamp() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _timeStamp;
            }
            
            
            
            inline void setTimeStamp(const double& timeStamp) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _timeStamp = timeStamp;
            }
            
            
            
            inline double getPreviousTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _previousTimeStepSize;
            }
            
            
            
            inline void setPreviousTimeStepSize(const double& previousTimeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _previousTimeStepSize = previousTimeStepSize;
            }
            
            
            
            inline int getSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _solution;
            }
            
            
            
            inline void setSolution(const int& solution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _solution = solution;
            }
            
            
            
            inline int getSolutionAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _solutionAverages;
            }
            
            
            
            inline void setSolutionAverages(const int& solutionAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _solutionAverages = solutionAverages;
            }
            
            
            
            inline int getSolutionCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _solutionCompressed;
            }
            
            
            
            inline void setSolutionCompressed(const int& solutionCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _solutionCompressed = solutionCompressed;
            }
            
            
            
            inline int getPreviousSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _previousSolution;
            }
            
            
            
            inline void setPreviousSolution(const int& previousSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _previousSolution = previousSolution;
            }
            
            
            
            inline int getPreviousSolutionAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _previousSolutionAverages;
            }
            
            
            
            inline void setPreviousSolutionAverages(const int& previousSolutionAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _previousSolutionAverages = previousSolutionAverages;
            }
            
            
            
            inline int getPreviousSolutionCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _previousSolutionCompressed;
            }
            
            
            
            inline void setPreviousSolutionCompressed(const int& previousSolutionCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _previousSolutionCompressed = previousSolutionCompressed;
            }
            
            
            
            inline int getExtrapolatedSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _extrapolatedSolution;
            }
            
            
            
            inline void setExtrapolatedSolution(const int& extrapolatedSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _extrapolatedSolution = extrapolatedSolution;
            }
            
            
            
            inline int getExtrapolatedSolutionAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _extrapolatedSolutionAverages;
            }
            
            
            
            inline void setExtrapolatedSolutionAverages(const int& extrapolatedSolutionAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _extrapolatedSolutionAverages = extrapolatedSolutionAverages;
            }
            
            
            
            inline int getExtrapolatedSolutionCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _extrapolatedSolutionCompressed;
            }
            
            
            
            inline void setExtrapolatedSolutionCompressed(const int& extrapolatedSolutionCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _extrapolatedSolutionCompressed = extrapolatedSolutionCompressed;
            }
            
            
            
            inline CompressionState getCompressionState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask =  (1 << (2)) - 1;
   mask = static_cast<int>(mask << (0));
   int tmp = static_cast<int>(_packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (0));
   assertion(( tmp >= 0 &&  tmp <= 2));
   return (CompressionState) tmp;
            }
            
            
            
            inline void setCompressionState(const CompressionState& compressionState) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion((compressionState >= 0 && compressionState <= 2));
   int mask =  (1 << (2)) - 1;
   mask = static_cast<int>(mask << (0));
   _packedRecords0 = static_cast<int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<int>(_packedRecords0 | static_cast<int>(compressionState) << (0));
            }
            
            
            
            inline int getBytesPerDoFInPreviousSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (2));
   int tmp = static_cast<int>(_packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (2));
   tmp = tmp + 1;
   assertion(( tmp >= 1 &&  tmp <= 7));
   return (int) tmp;
            }
            
            
            
            inline void setBytesPerDoFInPreviousSolution(const int& bytesPerDoFInPreviousSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion((bytesPerDoFInPreviousSolution >= 1 && bytesPerDoFInPreviousSolution <= 7));
   int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (2));
   _packedRecords0 = static_cast<int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<int>(_packedRecords0 | (static_cast<int>(bytesPerDoFInPreviousSolution) - 1) << (2));
            }
            
            
            
            inline int getBytesPerDoFInSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (5));
   int tmp = static_cast<int>(_packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (5));
   tmp = tmp + 1;
   assertion(( tmp >= 1 &&  tmp <= 7));
   return (int) tmp;
            }
            
            
            
            inline void setBytesPerDoFInSolution(const int& bytesPerDoFInSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion((bytesPerDoFInSolution >= 1 && bytesPerDoFInSolution <= 7));
   int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (5));
   _packedRecords0 = static_cast<int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<int>(_packedRecords0 | (static_cast<int>(bytesPerDoFInSolution) - 1) << (5));
            }
            
            
            
            inline int getBytesPerDoFInExtrapolatedSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (8));
   int tmp = static_cast<int>(_packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (8));
   tmp = tmp + 1;
   assertion(( tmp >= 1 &&  tmp <= 7));
   return (int) tmp;
            }
            
            
            
            inline void setBytesPerDoFInExtrapolatedSolution(const int& bytesPerDoFInExtrapolatedSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion((bytesPerDoFInExtrapolatedSolution >= 1 && bytesPerDoFInExtrapolatedSolution <= 7));
   int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (8));
   _packedRecords0 = static_cast<int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<int>(_packedRecords0 | (static_cast<int>(bytesPerDoFInExtrapolatedSolution) - 1) << (8));
            }
            
            
            
            inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _level;
            }
            
            
            
            inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _level = level;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<DIMENSIONS,double> getOffset() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _offset;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setOffset(const tarch::la::Vector<DIMENSIONS,double>& offset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _offset = (offset);
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<DIMENSIONS,double> getSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _size;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setSize(const tarch::la::Vector<DIMENSIONS,double>& size) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _size = (size);
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline std::bitset<DIMENSIONS_TIMES_TWO> getNeighbourMergePerformed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _neighbourMergePerformed;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setNeighbourMergePerformed(const std::bitset<DIMENSIONS_TIMES_TWO>& neighbourMergePerformed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _neighbourMergePerformed = (neighbourMergePerformed);
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline std::bitset<DIMENSIONS_TIMES_TWO> getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _isInside;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setIsInside(const std::bitset<DIMENSIONS_TIMES_TWO>& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _isInside = (isInside);
            }
            
            
            
            inline bool getOneRemoteBoundaryNeighbourIsOfTypeCell() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _oneRemoteBoundaryNeighbourIsOfTypeCell;
            }
            
            
            
            inline void setOneRemoteBoundaryNeighbourIsOfTypeCell(const bool& oneRemoteBoundaryNeighbourIsOfTypeCell) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _oneRemoteBoundaryNeighbourIsOfTypeCell = oneRemoteBoundaryNeighbourIsOfTypeCell;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> getFaceDataExchangeCounter() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _faceDataExchangeCounter;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setFaceDataExchangeCounter(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& faceDataExchangeCounter) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _faceDataExchangeCounter = (faceDataExchangeCounter);
            }
            
            
            
            inline Type getType() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _type;
            }
            
            
            
            inline void setType(const Type& type) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _type = type;
            }
            
            
            
            inline int getParentIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _parentIndex;
            }
            
            
            
            inline void setParentIndex(const int& parentIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _parentIndex = parentIndex;
            }
            
            
            
            inline RefinementEvent getRefinementEvent() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _refinementEvent;
            }
            
            
            
            inline void setRefinementEvent(const RefinementEvent& refinementEvent) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _refinementEvent = refinementEvent;
            }
            
            
            
         };
         private: 
            PersistentRecords _persistentRecords;
            
         public:
            /**
             * Generated
             */
            FiniteVolumesCellDescriptionPacked();
            
            /**
             * Generated
             */
            FiniteVolumesCellDescriptionPacked(const PersistentRecords& persistentRecords);
            
            /**
             * Generated
             */
            FiniteVolumesCellDescriptionPacked(const int& solverNumber, const double& timeStepSize, const double& timeStamp, const double& previousTimeStepSize, const int& solution, const int& solutionAverages, const int& solutionCompressed, const int& previousSolution, const int& previousSolutionAverages, const int& previousSolutionCompressed, const int& extrapolatedSolution, const int& extrapolatedSolutionAverages, const int& extrapolatedSolutionCompressed, const CompressionState& compressionState, const int& bytesPerDoFInPreviousSolution, const int& bytesPerDoFInSolution, const int& bytesPerDoFInExtrapolatedSolution, const int& level, const tarch::la::Vector<DIMENSIONS,double>& offset, const tarch::la::Vector<DIMENSIONS,double>& size, const std::bitset<DIMENSIONS_TIMES_TWO>& neighbourMergePerformed, const std::bitset<DIMENSIONS_TIMES_TWO>& isInside, const bool& oneRemoteBoundaryNeighbourIsOfTypeCell, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& faceDataExchangeCounter, const Type& type, const int& parentIndex, const RefinementEvent& refinementEvent);
            
            /**
             * Generated
             */
            ~FiniteVolumesCellDescriptionPacked();
            
            
            inline int getSolverNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._solverNumber;
            }
            
            
            
            inline void setSolverNumber(const int& solverNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._solverNumber = solverNumber;
            }
            
            
            
            inline double getTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._timeStepSize;
            }
            
            
            
            inline void setTimeStepSize(const double& timeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._timeStepSize = timeStepSize;
            }
            
            
            
            inline double getTimeStamp() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._timeStamp;
            }
            
            
            
            inline void setTimeStamp(const double& timeStamp) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._timeStamp = timeStamp;
            }
            
            
            
            inline double getPreviousTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._previousTimeStepSize;
            }
            
            
            
            inline void setPreviousTimeStepSize(const double& previousTimeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._previousTimeStepSize = previousTimeStepSize;
            }
            
            
            
            inline int getSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._solution;
            }
            
            
            
            inline void setSolution(const int& solution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._solution = solution;
            }
            
            
            
            inline int getSolutionAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._solutionAverages;
            }
            
            
            
            inline void setSolutionAverages(const int& solutionAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._solutionAverages = solutionAverages;
            }
            
            
            
            inline int getSolutionCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._solutionCompressed;
            }
            
            
            
            inline void setSolutionCompressed(const int& solutionCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._solutionCompressed = solutionCompressed;
            }
            
            
            
            inline int getPreviousSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._previousSolution;
            }
            
            
            
            inline void setPreviousSolution(const int& previousSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._previousSolution = previousSolution;
            }
            
            
            
            inline int getPreviousSolutionAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._previousSolutionAverages;
            }
            
            
            
            inline void setPreviousSolutionAverages(const int& previousSolutionAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._previousSolutionAverages = previousSolutionAverages;
            }
            
            
            
            inline int getPreviousSolutionCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._previousSolutionCompressed;
            }
            
            
            
            inline void setPreviousSolutionCompressed(const int& previousSolutionCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._previousSolutionCompressed = previousSolutionCompressed;
            }
            
            
            
            inline int getExtrapolatedSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._extrapolatedSolution;
            }
            
            
            
            inline void setExtrapolatedSolution(const int& extrapolatedSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._extrapolatedSolution = extrapolatedSolution;
            }
            
            
            
            inline int getExtrapolatedSolutionAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._extrapolatedSolutionAverages;
            }
            
            
            
            inline void setExtrapolatedSolutionAverages(const int& extrapolatedSolutionAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._extrapolatedSolutionAverages = extrapolatedSolutionAverages;
            }
            
            
            
            inline int getExtrapolatedSolutionCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._extrapolatedSolutionCompressed;
            }
            
            
            
            inline void setExtrapolatedSolutionCompressed(const int& extrapolatedSolutionCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._extrapolatedSolutionCompressed = extrapolatedSolutionCompressed;
            }
            
            
            
            inline CompressionState getCompressionState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask =  (1 << (2)) - 1;
   mask = static_cast<int>(mask << (0));
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (0));
   assertion(( tmp >= 0 &&  tmp <= 2));
   return (CompressionState) tmp;
            }
            
            
            
            inline void setCompressionState(const CompressionState& compressionState) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion((compressionState >= 0 && compressionState <= 2));
   int mask =  (1 << (2)) - 1;
   mask = static_cast<int>(mask << (0));
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 | static_cast<int>(compressionState) << (0));
            }
            
            
            
            inline int getBytesPerDoFInPreviousSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (2));
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (2));
   tmp = tmp + 1;
   assertion(( tmp >= 1 &&  tmp <= 7));
   return (int) tmp;
            }
            
            
            
            inline void setBytesPerDoFInPreviousSolution(const int& bytesPerDoFInPreviousSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion((bytesPerDoFInPreviousSolution >= 1 && bytesPerDoFInPreviousSolution <= 7));
   int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (2));
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 | (static_cast<int>(bytesPerDoFInPreviousSolution) - 1) << (2));
            }
            
            
            
            inline int getBytesPerDoFInSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (5));
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (5));
   tmp = tmp + 1;
   assertion(( tmp >= 1 &&  tmp <= 7));
   return (int) tmp;
            }
            
            
            
            inline void setBytesPerDoFInSolution(const int& bytesPerDoFInSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion((bytesPerDoFInSolution >= 1 && bytesPerDoFInSolution <= 7));
   int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (5));
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 | (static_cast<int>(bytesPerDoFInSolution) - 1) << (5));
            }
            
            
            
            inline int getBytesPerDoFInExtrapolatedSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (8));
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (8));
   tmp = tmp + 1;
   assertion(( tmp >= 1 &&  tmp <= 7));
   return (int) tmp;
            }
            
            
            
            inline void setBytesPerDoFInExtrapolatedSolution(const int& bytesPerDoFInExtrapolatedSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion((bytesPerDoFInExtrapolatedSolution >= 1 && bytesPerDoFInExtrapolatedSolution <= 7));
   int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (8));
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 | (static_cast<int>(bytesPerDoFInExtrapolatedSolution) - 1) << (8));
            }
            
            
            
            inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._level;
            }
            
            
            
            inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._level = level;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<DIMENSIONS,double> getOffset() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._offset;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setOffset(const tarch::la::Vector<DIMENSIONS,double>& offset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._offset = (offset);
            }
            
            
            
            inline double getOffset(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS);
               return _persistentRecords._offset[elementIndex];
               
            }
            
            
            
            inline void setOffset(int elementIndex, const double& offset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS);
               _persistentRecords._offset[elementIndex]= offset;
               
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<DIMENSIONS,double> getSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._size;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setSize(const tarch::la::Vector<DIMENSIONS,double>& size) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._size = (size);
            }
            
            
            
            inline double getSize(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS);
               return _persistentRecords._size[elementIndex];
               
            }
            
            
            
            inline void setSize(int elementIndex, const double& size) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS);
               _persistentRecords._size[elementIndex]= size;
               
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline std::bitset<DIMENSIONS_TIMES_TWO> getNeighbourMergePerformed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._neighbourMergePerformed;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setNeighbourMergePerformed(const std::bitset<DIMENSIONS_TIMES_TWO>& neighbourMergePerformed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._neighbourMergePerformed = (neighbourMergePerformed);
            }
            
            
            
            inline bool getNeighbourMergePerformed(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS_TIMES_TWO);
               return _persistentRecords._neighbourMergePerformed[elementIndex];
               
            }
            
            
            
            inline void setNeighbourMergePerformed(int elementIndex, const bool& neighbourMergePerformed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS_TIMES_TWO);
               _persistentRecords._neighbourMergePerformed[elementIndex]= neighbourMergePerformed;
               
            }
            
            
            
            inline void flipNeighbourMergePerformed(int elementIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS_TIMES_TWO);
               _persistentRecords._neighbourMergePerformed.flip(elementIndex);
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline std::bitset<DIMENSIONS_TIMES_TWO> getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._isInside;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setIsInside(const std::bitset<DIMENSIONS_TIMES_TWO>& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._isInside = (isInside);
            }
            
            
            
            inline bool getIsInside(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS_TIMES_TWO);
               return _persistentRecords._isInside[elementIndex];
               
            }
            
            
            
            inline void setIsInside(int elementIndex, const bool& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS_TIMES_TWO);
               _persistentRecords._isInside[elementIndex]= isInside;
               
            }
            
            
            
            inline void flipIsInside(int elementIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS_TIMES_TWO);
               _persistentRecords._isInside.flip(elementIndex);
            }
            
            
            
            inline bool getOneRemoteBoundaryNeighbourIsOfTypeCell() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._oneRemoteBoundaryNeighbourIsOfTypeCell;
            }
            
            
            
            inline void setOneRemoteBoundaryNeighbourIsOfTypeCell(const bool& oneRemoteBoundaryNeighbourIsOfTypeCell) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._oneRemoteBoundaryNeighbourIsOfTypeCell = oneRemoteBoundaryNeighbourIsOfTypeCell;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> getFaceDataExchangeCounter() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._faceDataExchangeCounter;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setFaceDataExchangeCounter(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& faceDataExchangeCounter) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._faceDataExchangeCounter = (faceDataExchangeCounter);
            }
            
            
            
            inline int getFaceDataExchangeCounter(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS_TIMES_TWO);
               return _persistentRecords._faceDataExchangeCounter[elementIndex];
               
            }
            
            
            
            inline void setFaceDataExchangeCounter(int elementIndex, const int& faceDataExchangeCounter) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS_TIMES_TWO);
               _persistentRecords._faceDataExchangeCounter[elementIndex]= faceDataExchangeCounter;
               
            }
            
            
            
            inline Type getType() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._type;
            }
            
            
            
            inline void setType(const Type& type) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._type = type;
            }
            
            
            
            inline int getParentIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._parentIndex;
            }
            
            
            
            inline void setParentIndex(const int& parentIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._parentIndex = parentIndex;
            }
            
            
            
            inline RefinementEvent getRefinementEvent() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._refinementEvent;
            }
            
            
            
            inline void setRefinementEvent(const RefinementEvent& refinementEvent) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._refinementEvent = refinementEvent;
            }
            
            
            /**
             * Generated
             */
            static std::string toString(const CompressionState& param);
            
            /**
             * Generated
             */
            static std::string getCompressionStateMapping();
            
            /**
             * Generated
             */
            static std::string toString(const Type& param);
            
            /**
             * Generated
             */
            static std::string getTypeMapping();
            
            /**
             * Generated
             */
            static std::string toString(const RefinementEvent& param);
            
            /**
             * Generated
             */
            static std::string getRefinementEventMapping();
            
            /**
             * Generated
             */
            std::string toString() const;
            
            /**
             * Generated
             */
            void toString(std::ostream& out) const;
            
            
            PersistentRecords getPersistentRecords() const;
            /**
             * Generated
             */
            FiniteVolumesCellDescription convert() const;
            
            
         #ifdef Parallel
            protected:
               static tarch::logging::Log _log;
               
            public:
               
               /**
                * Global that represents the mpi datatype.
                * There are two variants: Datatype identifies only those attributes marked with
                * parallelise. FullDatatype instead identifies the whole record with all fields.
                */
               static MPI_Datatype Datatype;
               static MPI_Datatype FullDatatype;
               
               /**
                * Initializes the data type for the mpi operations. Has to be called
                * before the very first send or receive operation is called.
                */
               static void initDatatype();
               
               static void shutdownDatatype();
               
               /**
                * @param communicateSleep -1 Data exchange through blocking mpi
                * @param communicateSleep  0 Data exchange through non-blocking mpi, i.e. pending messages are received via polling until MPI_Test succeeds
                * @param communicateSleep >0 Same as 0 but in addition, each unsuccessful MPI_Test is follows by an usleep
                */
               void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, int communicateSleep);
               
               void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, int communicateSleep);
               
               static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
               
               #endif
      
   };
   
   #ifdef PackedRecords
   #pragma pack (pop)
   #endif
   
   
   #elif !defined(Parallel)
      /**
       * @author This class is generated by DaStGen
       * 		   DataStructureGenerator (DaStGen)
       * 		   2007-2009 Wolfgang Eckhardt
       * 		   2012      Tobias Weinzierl
       *
       * 		   build date: 09-02-2014 14:40
       *
       * @date   26/09/2017 12:31
       */
      class exahype::records::FiniteVolumesCellDescription { 
         
         public:
            
            typedef exahype::records::FiniteVolumesCellDescriptionPacked Packed;
            
            enum CompressionState {
               Uncompressed = 0, CurrentlyProcessed = 1, Compressed = 2
            };
            
            enum RefinementEvent {
               None = 0, ErasingChildrenRequested = 1, ErasingChildren = 2, ChangeChildrenToDescendantsRequested = 3, ChangeChildrenToDescendants = 4, RefiningRequested = 5, Refining = 6, DeaugmentingChildrenRequestedTriggered = 7, DeaugmentingChildrenRequested = 8, DeaugmentingChildren = 9, AugmentingRequested = 10, Augmenting = 11
            };
            
            enum Type {
               Erased = 0, Ancestor = 1, Cell = 2, Descendant = 3
            };
            
            struct PersistentRecords {
               int _solverNumber;
               double _timeStepSize;
               double _timeStamp;
               double _previousTimeStepSize;
               int _solution;
               int _solutionAverages;
               int _solutionCompressed;
               int _previousSolution;
               int _previousSolutionAverages;
               int _previousSolutionCompressed;
               int _extrapolatedSolution;
               int _extrapolatedSolutionAverages;
               int _extrapolatedSolutionCompressed;
               CompressionState _compressionState;
               int _bytesPerDoFInPreviousSolution;
               int _bytesPerDoFInSolution;
               int _bytesPerDoFInExtrapolatedSolution;
               int _level;
               #ifdef UseManualAlignment
               tarch::la::Vector<DIMENSIONS,double> _offset __attribute__((aligned(VectorisationAlignment)));
               #else
               tarch::la::Vector<DIMENSIONS,double> _offset;
               #endif
               #ifdef UseManualAlignment
               tarch::la::Vector<DIMENSIONS,double> _size __attribute__((aligned(VectorisationAlignment)));
               #else
               tarch::la::Vector<DIMENSIONS,double> _size;
               #endif
               #ifdef UseManualAlignment
               std::bitset<DIMENSIONS_TIMES_TWO> _neighbourMergePerformed __attribute__((aligned(VectorisationAlignment)));
               #else
               std::bitset<DIMENSIONS_TIMES_TWO> _neighbourMergePerformed;
               #endif
               #ifdef UseManualAlignment
               std::bitset<DIMENSIONS_TIMES_TWO> _isInside __attribute__((aligned(VectorisationAlignment)));
               #else
               std::bitset<DIMENSIONS_TIMES_TWO> _isInside;
               #endif
               Type _type;
               int _parentIndex;
               RefinementEvent _refinementEvent;
               /**
                * Generated
                */
               PersistentRecords();
               
               /**
                * Generated
                */
               PersistentRecords(const int& solverNumber, const double& timeStepSize, const double& timeStamp, const double& previousTimeStepSize, const int& solution, const int& solutionAverages, const int& solutionCompressed, const int& previousSolution, const int& previousSolutionAverages, const int& previousSolutionCompressed, const int& extrapolatedSolution, const int& extrapolatedSolutionAverages, const int& extrapolatedSolutionCompressed, const CompressionState& compressionState, const int& bytesPerDoFInPreviousSolution, const int& bytesPerDoFInSolution, const int& bytesPerDoFInExtrapolatedSolution, const int& level, const tarch::la::Vector<DIMENSIONS,double>& offset, const tarch::la::Vector<DIMENSIONS,double>& size, const std::bitset<DIMENSIONS_TIMES_TWO>& neighbourMergePerformed, const std::bitset<DIMENSIONS_TIMES_TWO>& isInside, const Type& type, const int& parentIndex, const RefinementEvent& refinementEvent);
               
               
               inline int getSolverNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _solverNumber;
               }
               
               
               
               inline void setSolverNumber(const int& solverNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _solverNumber = solverNumber;
               }
               
               
               
               inline double getTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _timeStepSize;
               }
               
               
               
               inline void setTimeStepSize(const double& timeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _timeStepSize = timeStepSize;
               }
               
               
               
               inline double getTimeStamp() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _timeStamp;
               }
               
               
               
               inline void setTimeStamp(const double& timeStamp) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _timeStamp = timeStamp;
               }
               
               
               
               inline double getPreviousTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _previousTimeStepSize;
               }
               
               
               
               inline void setPreviousTimeStepSize(const double& previousTimeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _previousTimeStepSize = previousTimeStepSize;
               }
               
               
               
               inline int getSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _solution;
               }
               
               
               
               inline void setSolution(const int& solution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _solution = solution;
               }
               
               
               
               inline int getSolutionAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _solutionAverages;
               }
               
               
               
               inline void setSolutionAverages(const int& solutionAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _solutionAverages = solutionAverages;
               }
               
               
               
               inline int getSolutionCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _solutionCompressed;
               }
               
               
               
               inline void setSolutionCompressed(const int& solutionCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _solutionCompressed = solutionCompressed;
               }
               
               
               
               inline int getPreviousSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _previousSolution;
               }
               
               
               
               inline void setPreviousSolution(const int& previousSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _previousSolution = previousSolution;
               }
               
               
               
               inline int getPreviousSolutionAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _previousSolutionAverages;
               }
               
               
               
               inline void setPreviousSolutionAverages(const int& previousSolutionAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _previousSolutionAverages = previousSolutionAverages;
               }
               
               
               
               inline int getPreviousSolutionCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _previousSolutionCompressed;
               }
               
               
               
               inline void setPreviousSolutionCompressed(const int& previousSolutionCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _previousSolutionCompressed = previousSolutionCompressed;
               }
               
               
               
               inline int getExtrapolatedSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _extrapolatedSolution;
               }
               
               
               
               inline void setExtrapolatedSolution(const int& extrapolatedSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _extrapolatedSolution = extrapolatedSolution;
               }
               
               
               
               inline int getExtrapolatedSolutionAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _extrapolatedSolutionAverages;
               }
               
               
               
               inline void setExtrapolatedSolutionAverages(const int& extrapolatedSolutionAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _extrapolatedSolutionAverages = extrapolatedSolutionAverages;
               }
               
               
               
               inline int getExtrapolatedSolutionCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _extrapolatedSolutionCompressed;
               }
               
               
               
               inline void setExtrapolatedSolutionCompressed(const int& extrapolatedSolutionCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _extrapolatedSolutionCompressed = extrapolatedSolutionCompressed;
               }
               
               
               
               inline CompressionState getCompressionState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _compressionState;
               }
               
               
               
               inline void setCompressionState(const CompressionState& compressionState) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _compressionState = compressionState;
               }
               
               
               
               inline int getBytesPerDoFInPreviousSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _bytesPerDoFInPreviousSolution;
               }
               
               
               
               inline void setBytesPerDoFInPreviousSolution(const int& bytesPerDoFInPreviousSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _bytesPerDoFInPreviousSolution = bytesPerDoFInPreviousSolution;
               }
               
               
               
               inline int getBytesPerDoFInSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _bytesPerDoFInSolution;
               }
               
               
               
               inline void setBytesPerDoFInSolution(const int& bytesPerDoFInSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _bytesPerDoFInSolution = bytesPerDoFInSolution;
               }
               
               
               
               inline int getBytesPerDoFInExtrapolatedSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _bytesPerDoFInExtrapolatedSolution;
               }
               
               
               
               inline void setBytesPerDoFInExtrapolatedSolution(const int& bytesPerDoFInExtrapolatedSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _bytesPerDoFInExtrapolatedSolution = bytesPerDoFInExtrapolatedSolution;
               }
               
               
               
               inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _level;
               }
               
               
               
               inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _level = level;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<DIMENSIONS,double> getOffset() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _offset;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setOffset(const tarch::la::Vector<DIMENSIONS,double>& offset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _offset = (offset);
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<DIMENSIONS,double> getSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _size;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setSize(const tarch::la::Vector<DIMENSIONS,double>& size) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _size = (size);
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline std::bitset<DIMENSIONS_TIMES_TWO> getNeighbourMergePerformed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _neighbourMergePerformed;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setNeighbourMergePerformed(const std::bitset<DIMENSIONS_TIMES_TWO>& neighbourMergePerformed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _neighbourMergePerformed = (neighbourMergePerformed);
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline std::bitset<DIMENSIONS_TIMES_TWO> getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _isInside;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setIsInside(const std::bitset<DIMENSIONS_TIMES_TWO>& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _isInside = (isInside);
               }
               
               
               
               inline Type getType() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _type;
               }
               
               
               
               inline void setType(const Type& type) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _type = type;
               }
               
               
               
               inline int getParentIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _parentIndex;
               }
               
               
               
               inline void setParentIndex(const int& parentIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _parentIndex = parentIndex;
               }
               
               
               
               inline RefinementEvent getRefinementEvent() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _refinementEvent;
               }
               
               
               
               inline void setRefinementEvent(const RefinementEvent& refinementEvent) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _refinementEvent = refinementEvent;
               }
               
               
               
            };
            private: 
               PersistentRecords _persistentRecords;
               
            public:
               /**
                * Generated
                */
               FiniteVolumesCellDescription();
               
               /**
                * Generated
                */
               FiniteVolumesCellDescription(const PersistentRecords& persistentRecords);
               
               /**
                * Generated
                */
               FiniteVolumesCellDescription(const int& solverNumber, const double& timeStepSize, const double& timeStamp, const double& previousTimeStepSize, const int& solution, const int& solutionAverages, const int& solutionCompressed, const int& previousSolution, const int& previousSolutionAverages, const int& previousSolutionCompressed, const int& extrapolatedSolution, const int& extrapolatedSolutionAverages, const int& extrapolatedSolutionCompressed, const CompressionState& compressionState, const int& bytesPerDoFInPreviousSolution, const int& bytesPerDoFInSolution, const int& bytesPerDoFInExtrapolatedSolution, const int& level, const tarch::la::Vector<DIMENSIONS,double>& offset, const tarch::la::Vector<DIMENSIONS,double>& size, const std::bitset<DIMENSIONS_TIMES_TWO>& neighbourMergePerformed, const std::bitset<DIMENSIONS_TIMES_TWO>& isInside, const Type& type, const int& parentIndex, const RefinementEvent& refinementEvent);
               
               /**
                * Generated
                */
               ~FiniteVolumesCellDescription();
               
               
               inline int getSolverNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._solverNumber;
               }
               
               
               
               inline void setSolverNumber(const int& solverNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._solverNumber = solverNumber;
               }
               
               
               
               inline double getTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._timeStepSize;
               }
               
               
               
               inline void setTimeStepSize(const double& timeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._timeStepSize = timeStepSize;
               }
               
               
               
               inline double getTimeStamp() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._timeStamp;
               }
               
               
               
               inline void setTimeStamp(const double& timeStamp) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._timeStamp = timeStamp;
               }
               
               
               
               inline double getPreviousTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._previousTimeStepSize;
               }
               
               
               
               inline void setPreviousTimeStepSize(const double& previousTimeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._previousTimeStepSize = previousTimeStepSize;
               }
               
               
               
               inline int getSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._solution;
               }
               
               
               
               inline void setSolution(const int& solution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._solution = solution;
               }
               
               
               
               inline int getSolutionAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._solutionAverages;
               }
               
               
               
               inline void setSolutionAverages(const int& solutionAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._solutionAverages = solutionAverages;
               }
               
               
               
               inline int getSolutionCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._solutionCompressed;
               }
               
               
               
               inline void setSolutionCompressed(const int& solutionCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._solutionCompressed = solutionCompressed;
               }
               
               
               
               inline int getPreviousSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._previousSolution;
               }
               
               
               
               inline void setPreviousSolution(const int& previousSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._previousSolution = previousSolution;
               }
               
               
               
               inline int getPreviousSolutionAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._previousSolutionAverages;
               }
               
               
               
               inline void setPreviousSolutionAverages(const int& previousSolutionAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._previousSolutionAverages = previousSolutionAverages;
               }
               
               
               
               inline int getPreviousSolutionCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._previousSolutionCompressed;
               }
               
               
               
               inline void setPreviousSolutionCompressed(const int& previousSolutionCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._previousSolutionCompressed = previousSolutionCompressed;
               }
               
               
               
               inline int getExtrapolatedSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._extrapolatedSolution;
               }
               
               
               
               inline void setExtrapolatedSolution(const int& extrapolatedSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._extrapolatedSolution = extrapolatedSolution;
               }
               
               
               
               inline int getExtrapolatedSolutionAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._extrapolatedSolutionAverages;
               }
               
               
               
               inline void setExtrapolatedSolutionAverages(const int& extrapolatedSolutionAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._extrapolatedSolutionAverages = extrapolatedSolutionAverages;
               }
               
               
               
               inline int getExtrapolatedSolutionCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._extrapolatedSolutionCompressed;
               }
               
               
               
               inline void setExtrapolatedSolutionCompressed(const int& extrapolatedSolutionCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._extrapolatedSolutionCompressed = extrapolatedSolutionCompressed;
               }
               
               
               
               inline CompressionState getCompressionState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._compressionState;
               }
               
               
               
               inline void setCompressionState(const CompressionState& compressionState) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._compressionState = compressionState;
               }
               
               
               
               inline int getBytesPerDoFInPreviousSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._bytesPerDoFInPreviousSolution;
               }
               
               
               
               inline void setBytesPerDoFInPreviousSolution(const int& bytesPerDoFInPreviousSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._bytesPerDoFInPreviousSolution = bytesPerDoFInPreviousSolution;
               }
               
               
               
               inline int getBytesPerDoFInSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._bytesPerDoFInSolution;
               }
               
               
               
               inline void setBytesPerDoFInSolution(const int& bytesPerDoFInSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._bytesPerDoFInSolution = bytesPerDoFInSolution;
               }
               
               
               
               inline int getBytesPerDoFInExtrapolatedSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._bytesPerDoFInExtrapolatedSolution;
               }
               
               
               
               inline void setBytesPerDoFInExtrapolatedSolution(const int& bytesPerDoFInExtrapolatedSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._bytesPerDoFInExtrapolatedSolution = bytesPerDoFInExtrapolatedSolution;
               }
               
               
               
               inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._level;
               }
               
               
               
               inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._level = level;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<DIMENSIONS,double> getOffset() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._offset;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setOffset(const tarch::la::Vector<DIMENSIONS,double>& offset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._offset = (offset);
               }
               
               
               
               inline double getOffset(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS);
                  return _persistentRecords._offset[elementIndex];
                  
               }
               
               
               
               inline void setOffset(int elementIndex, const double& offset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS);
                  _persistentRecords._offset[elementIndex]= offset;
                  
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<DIMENSIONS,double> getSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._size;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setSize(const tarch::la::Vector<DIMENSIONS,double>& size) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._size = (size);
               }
               
               
               
               inline double getSize(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS);
                  return _persistentRecords._size[elementIndex];
                  
               }
               
               
               
               inline void setSize(int elementIndex, const double& size) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS);
                  _persistentRecords._size[elementIndex]= size;
                  
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline std::bitset<DIMENSIONS_TIMES_TWO> getNeighbourMergePerformed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._neighbourMergePerformed;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setNeighbourMergePerformed(const std::bitset<DIMENSIONS_TIMES_TWO>& neighbourMergePerformed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._neighbourMergePerformed = (neighbourMergePerformed);
               }
               
               
               
               inline bool getNeighbourMergePerformed(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                  return _persistentRecords._neighbourMergePerformed[elementIndex];
                  
               }
               
               
               
               inline void setNeighbourMergePerformed(int elementIndex, const bool& neighbourMergePerformed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                  _persistentRecords._neighbourMergePerformed[elementIndex]= neighbourMergePerformed;
                  
               }
               
               
               
               inline void flipNeighbourMergePerformed(int elementIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                  _persistentRecords._neighbourMergePerformed.flip(elementIndex);
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline std::bitset<DIMENSIONS_TIMES_TWO> getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._isInside;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setIsInside(const std::bitset<DIMENSIONS_TIMES_TWO>& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._isInside = (isInside);
               }
               
               
               
               inline bool getIsInside(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                  return _persistentRecords._isInside[elementIndex];
                  
               }
               
               
               
               inline void setIsInside(int elementIndex, const bool& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                  _persistentRecords._isInside[elementIndex]= isInside;
                  
               }
               
               
               
               inline void flipIsInside(int elementIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                  _persistentRecords._isInside.flip(elementIndex);
               }
               
               
               
               inline Type getType() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._type;
               }
               
               
               
               inline void setType(const Type& type) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._type = type;
               }
               
               
               
               inline int getParentIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._parentIndex;
               }
               
               
               
               inline void setParentIndex(const int& parentIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._parentIndex = parentIndex;
               }
               
               
               
               inline RefinementEvent getRefinementEvent() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._refinementEvent;
               }
               
               
               
               inline void setRefinementEvent(const RefinementEvent& refinementEvent) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._refinementEvent = refinementEvent;
               }
               
               
               /**
                * Generated
                */
               static std::string toString(const CompressionState& param);
               
               /**
                * Generated
                */
               static std::string getCompressionStateMapping();
               
               /**
                * Generated
                */
               static std::string toString(const RefinementEvent& param);
               
               /**
                * Generated
                */
               static std::string getRefinementEventMapping();
               
               /**
                * Generated
                */
               static std::string toString(const Type& param);
               
               /**
                * Generated
                */
               static std::string getTypeMapping();
               
               /**
                * Generated
                */
               std::string toString() const;
               
               /**
                * Generated
                */
               void toString(std::ostream& out) const;
               
               
               PersistentRecords getPersistentRecords() const;
               /**
                * Generated
                */
               FiniteVolumesCellDescriptionPacked convert() const;
               
               
            #ifdef Parallel
               protected:
                  static tarch::logging::Log _log;
                  
               public:
                  
                  /**
                   * Global that represents the mpi datatype.
                   * There are two variants: Datatype identifies only those attributes marked with
                   * parallelise. FullDatatype instead identifies the whole record with all fields.
                   */
                  static MPI_Datatype Datatype;
                  static MPI_Datatype FullDatatype;
                  
                  /**
                   * Initializes the data type for the mpi operations. Has to be called
                   * before the very first send or receive operation is called.
                   */
                  static void initDatatype();
                  
                  static void shutdownDatatype();
                  
                  /**
                   * @param communicateSleep -1 Data exchange through blocking mpi
                   * @param communicateSleep  0 Data exchange through non-blocking mpi, i.e. pending messages are received via polling until MPI_Test succeeds
                   * @param communicateSleep >0 Same as 0 but in addition, each unsuccessful MPI_Test is follows by an usleep
                   */
                  void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, int communicateSleep);
                  
                  void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, int communicateSleep);
                  
                  static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                  
                  #endif
         
      };
      
      #ifndef DaStGenPackedPadding
        #define DaStGenPackedPadding 1      // 32 bit version
        // #define DaStGenPackedPadding 2   // 64 bit version
      #endif
      
      
      #ifdef PackedRecords
         #pragma pack (push, DaStGenPackedPadding)
      #endif
      
      /**
       * @author This class is generated by DaStGen
       * 		   DataStructureGenerator (DaStGen)
       * 		   2007-2009 Wolfgang Eckhardt
       * 		   2012      Tobias Weinzierl
       *
       * 		   build date: 09-02-2014 14:40
       *
       * @date   26/09/2017 12:31
       */
      class exahype::records::FiniteVolumesCellDescriptionPacked { 
         
         public:
            
            typedef exahype::records::FiniteVolumesCellDescription::CompressionState CompressionState;
            
            typedef exahype::records::FiniteVolumesCellDescription::Type Type;
            
            typedef exahype::records::FiniteVolumesCellDescription::RefinementEvent RefinementEvent;
            
            struct PersistentRecords {
               int _solverNumber;
               double _timeStepSize;
               double _timeStamp;
               double _previousTimeStepSize;
               int _solution;
               int _solutionAverages;
               int _solutionCompressed;
               int _previousSolution;
               int _previousSolutionAverages;
               int _previousSolutionCompressed;
               int _extrapolatedSolution;
               int _extrapolatedSolutionAverages;
               int _extrapolatedSolutionCompressed;
               int _level;
               tarch::la::Vector<DIMENSIONS,double> _offset;
               tarch::la::Vector<DIMENSIONS,double> _size;
               std::bitset<DIMENSIONS_TIMES_TWO> _neighbourMergePerformed;
               std::bitset<DIMENSIONS_TIMES_TWO> _isInside;
               Type _type;
               int _parentIndex;
               RefinementEvent _refinementEvent;
               
               /** mapping of records:
               || Member 	|| startbit 	|| length
                |  compressionState	| startbit 0	| #bits 2
                |  bytesPerDoFInPreviousSolution	| startbit 2	| #bits 3
                |  bytesPerDoFInSolution	| startbit 5	| #bits 3
                |  bytesPerDoFInExtrapolatedSolution	| startbit 8	| #bits 3
                */
               int _packedRecords0;
               
               /**
                * Generated
                */
               PersistentRecords();
               
               /**
                * Generated
                */
               PersistentRecords(const int& solverNumber, const double& timeStepSize, const double& timeStamp, const double& previousTimeStepSize, const int& solution, const int& solutionAverages, const int& solutionCompressed, const int& previousSolution, const int& previousSolutionAverages, const int& previousSolutionCompressed, const int& extrapolatedSolution, const int& extrapolatedSolutionAverages, const int& extrapolatedSolutionCompressed, const CompressionState& compressionState, const int& bytesPerDoFInPreviousSolution, const int& bytesPerDoFInSolution, const int& bytesPerDoFInExtrapolatedSolution, const int& level, const tarch::la::Vector<DIMENSIONS,double>& offset, const tarch::la::Vector<DIMENSIONS,double>& size, const std::bitset<DIMENSIONS_TIMES_TWO>& neighbourMergePerformed, const std::bitset<DIMENSIONS_TIMES_TWO>& isInside, const Type& type, const int& parentIndex, const RefinementEvent& refinementEvent);
               
               
               inline int getSolverNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _solverNumber;
               }
               
               
               
               inline void setSolverNumber(const int& solverNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _solverNumber = solverNumber;
               }
               
               
               
               inline double getTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _timeStepSize;
               }
               
               
               
               inline void setTimeStepSize(const double& timeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _timeStepSize = timeStepSize;
               }
               
               
               
               inline double getTimeStamp() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _timeStamp;
               }
               
               
               
               inline void setTimeStamp(const double& timeStamp) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _timeStamp = timeStamp;
               }
               
               
               
               inline double getPreviousTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _previousTimeStepSize;
               }
               
               
               
               inline void setPreviousTimeStepSize(const double& previousTimeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _previousTimeStepSize = previousTimeStepSize;
               }
               
               
               
               inline int getSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _solution;
               }
               
               
               
               inline void setSolution(const int& solution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _solution = solution;
               }
               
               
               
               inline int getSolutionAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _solutionAverages;
               }
               
               
               
               inline void setSolutionAverages(const int& solutionAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _solutionAverages = solutionAverages;
               }
               
               
               
               inline int getSolutionCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _solutionCompressed;
               }
               
               
               
               inline void setSolutionCompressed(const int& solutionCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _solutionCompressed = solutionCompressed;
               }
               
               
               
               inline int getPreviousSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _previousSolution;
               }
               
               
               
               inline void setPreviousSolution(const int& previousSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _previousSolution = previousSolution;
               }
               
               
               
               inline int getPreviousSolutionAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _previousSolutionAverages;
               }
               
               
               
               inline void setPreviousSolutionAverages(const int& previousSolutionAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _previousSolutionAverages = previousSolutionAverages;
               }
               
               
               
               inline int getPreviousSolutionCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _previousSolutionCompressed;
               }
               
               
               
               inline void setPreviousSolutionCompressed(const int& previousSolutionCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _previousSolutionCompressed = previousSolutionCompressed;
               }
               
               
               
               inline int getExtrapolatedSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _extrapolatedSolution;
               }
               
               
               
               inline void setExtrapolatedSolution(const int& extrapolatedSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _extrapolatedSolution = extrapolatedSolution;
               }
               
               
               
               inline int getExtrapolatedSolutionAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _extrapolatedSolutionAverages;
               }
               
               
               
               inline void setExtrapolatedSolutionAverages(const int& extrapolatedSolutionAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _extrapolatedSolutionAverages = extrapolatedSolutionAverages;
               }
               
               
               
               inline int getExtrapolatedSolutionCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _extrapolatedSolutionCompressed;
               }
               
               
               
               inline void setExtrapolatedSolutionCompressed(const int& extrapolatedSolutionCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _extrapolatedSolutionCompressed = extrapolatedSolutionCompressed;
               }
               
               
               
               inline CompressionState getCompressionState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask =  (1 << (2)) - 1;
   mask = static_cast<int>(mask << (0));
   int tmp = static_cast<int>(_packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (0));
   assertion(( tmp >= 0 &&  tmp <= 2));
   return (CompressionState) tmp;
               }
               
               
               
               inline void setCompressionState(const CompressionState& compressionState) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion((compressionState >= 0 && compressionState <= 2));
   int mask =  (1 << (2)) - 1;
   mask = static_cast<int>(mask << (0));
   _packedRecords0 = static_cast<int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<int>(_packedRecords0 | static_cast<int>(compressionState) << (0));
               }
               
               
               
               inline int getBytesPerDoFInPreviousSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (2));
   int tmp = static_cast<int>(_packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (2));
   tmp = tmp + 1;
   assertion(( tmp >= 1 &&  tmp <= 7));
   return (int) tmp;
               }
               
               
               
               inline void setBytesPerDoFInPreviousSolution(const int& bytesPerDoFInPreviousSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion((bytesPerDoFInPreviousSolution >= 1 && bytesPerDoFInPreviousSolution <= 7));
   int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (2));
   _packedRecords0 = static_cast<int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<int>(_packedRecords0 | (static_cast<int>(bytesPerDoFInPreviousSolution) - 1) << (2));
               }
               
               
               
               inline int getBytesPerDoFInSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (5));
   int tmp = static_cast<int>(_packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (5));
   tmp = tmp + 1;
   assertion(( tmp >= 1 &&  tmp <= 7));
   return (int) tmp;
               }
               
               
               
               inline void setBytesPerDoFInSolution(const int& bytesPerDoFInSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion((bytesPerDoFInSolution >= 1 && bytesPerDoFInSolution <= 7));
   int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (5));
   _packedRecords0 = static_cast<int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<int>(_packedRecords0 | (static_cast<int>(bytesPerDoFInSolution) - 1) << (5));
               }
               
               
               
               inline int getBytesPerDoFInExtrapolatedSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (8));
   int tmp = static_cast<int>(_packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (8));
   tmp = tmp + 1;
   assertion(( tmp >= 1 &&  tmp <= 7));
   return (int) tmp;
               }
               
               
               
               inline void setBytesPerDoFInExtrapolatedSolution(const int& bytesPerDoFInExtrapolatedSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion((bytesPerDoFInExtrapolatedSolution >= 1 && bytesPerDoFInExtrapolatedSolution <= 7));
   int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (8));
   _packedRecords0 = static_cast<int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<int>(_packedRecords0 | (static_cast<int>(bytesPerDoFInExtrapolatedSolution) - 1) << (8));
               }
               
               
               
               inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _level;
               }
               
               
               
               inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _level = level;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<DIMENSIONS,double> getOffset() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _offset;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setOffset(const tarch::la::Vector<DIMENSIONS,double>& offset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _offset = (offset);
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<DIMENSIONS,double> getSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _size;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setSize(const tarch::la::Vector<DIMENSIONS,double>& size) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _size = (size);
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline std::bitset<DIMENSIONS_TIMES_TWO> getNeighbourMergePerformed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _neighbourMergePerformed;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setNeighbourMergePerformed(const std::bitset<DIMENSIONS_TIMES_TWO>& neighbourMergePerformed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _neighbourMergePerformed = (neighbourMergePerformed);
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline std::bitset<DIMENSIONS_TIMES_TWO> getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _isInside;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setIsInside(const std::bitset<DIMENSIONS_TIMES_TWO>& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _isInside = (isInside);
               }
               
               
               
               inline Type getType() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _type;
               }
               
               
               
               inline void setType(const Type& type) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _type = type;
               }
               
               
               
               inline int getParentIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _parentIndex;
               }
               
               
               
               inline void setParentIndex(const int& parentIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _parentIndex = parentIndex;
               }
               
               
               
               inline RefinementEvent getRefinementEvent() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _refinementEvent;
               }
               
               
               
               inline void setRefinementEvent(const RefinementEvent& refinementEvent) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _refinementEvent = refinementEvent;
               }
               
               
               
            };
            private: 
               PersistentRecords _persistentRecords;
               
            public:
               /**
                * Generated
                */
               FiniteVolumesCellDescriptionPacked();
               
               /**
                * Generated
                */
               FiniteVolumesCellDescriptionPacked(const PersistentRecords& persistentRecords);
               
               /**
                * Generated
                */
               FiniteVolumesCellDescriptionPacked(const int& solverNumber, const double& timeStepSize, const double& timeStamp, const double& previousTimeStepSize, const int& solution, const int& solutionAverages, const int& solutionCompressed, const int& previousSolution, const int& previousSolutionAverages, const int& previousSolutionCompressed, const int& extrapolatedSolution, const int& extrapolatedSolutionAverages, const int& extrapolatedSolutionCompressed, const CompressionState& compressionState, const int& bytesPerDoFInPreviousSolution, const int& bytesPerDoFInSolution, const int& bytesPerDoFInExtrapolatedSolution, const int& level, const tarch::la::Vector<DIMENSIONS,double>& offset, const tarch::la::Vector<DIMENSIONS,double>& size, const std::bitset<DIMENSIONS_TIMES_TWO>& neighbourMergePerformed, const std::bitset<DIMENSIONS_TIMES_TWO>& isInside, const Type& type, const int& parentIndex, const RefinementEvent& refinementEvent);
               
               /**
                * Generated
                */
               ~FiniteVolumesCellDescriptionPacked();
               
               
               inline int getSolverNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._solverNumber;
               }
               
               
               
               inline void setSolverNumber(const int& solverNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._solverNumber = solverNumber;
               }
               
               
               
               inline double getTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._timeStepSize;
               }
               
               
               
               inline void setTimeStepSize(const double& timeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._timeStepSize = timeStepSize;
               }
               
               
               
               inline double getTimeStamp() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._timeStamp;
               }
               
               
               
               inline void setTimeStamp(const double& timeStamp) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._timeStamp = timeStamp;
               }
               
               
               
               inline double getPreviousTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._previousTimeStepSize;
               }
               
               
               
               inline void setPreviousTimeStepSize(const double& previousTimeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._previousTimeStepSize = previousTimeStepSize;
               }
               
               
               
               inline int getSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._solution;
               }
               
               
               
               inline void setSolution(const int& solution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._solution = solution;
               }
               
               
               
               inline int getSolutionAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._solutionAverages;
               }
               
               
               
               inline void setSolutionAverages(const int& solutionAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._solutionAverages = solutionAverages;
               }
               
               
               
               inline int getSolutionCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._solutionCompressed;
               }
               
               
               
               inline void setSolutionCompressed(const int& solutionCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._solutionCompressed = solutionCompressed;
               }
               
               
               
               inline int getPreviousSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._previousSolution;
               }
               
               
               
               inline void setPreviousSolution(const int& previousSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._previousSolution = previousSolution;
               }
               
               
               
               inline int getPreviousSolutionAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._previousSolutionAverages;
               }
               
               
               
               inline void setPreviousSolutionAverages(const int& previousSolutionAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._previousSolutionAverages = previousSolutionAverages;
               }
               
               
               
               inline int getPreviousSolutionCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._previousSolutionCompressed;
               }
               
               
               
               inline void setPreviousSolutionCompressed(const int& previousSolutionCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._previousSolutionCompressed = previousSolutionCompressed;
               }
               
               
               
               inline int getExtrapolatedSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._extrapolatedSolution;
               }
               
               
               
               inline void setExtrapolatedSolution(const int& extrapolatedSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._extrapolatedSolution = extrapolatedSolution;
               }
               
               
               
               inline int getExtrapolatedSolutionAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._extrapolatedSolutionAverages;
               }
               
               
               
               inline void setExtrapolatedSolutionAverages(const int& extrapolatedSolutionAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._extrapolatedSolutionAverages = extrapolatedSolutionAverages;
               }
               
               
               
               inline int getExtrapolatedSolutionCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._extrapolatedSolutionCompressed;
               }
               
               
               
               inline void setExtrapolatedSolutionCompressed(const int& extrapolatedSolutionCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._extrapolatedSolutionCompressed = extrapolatedSolutionCompressed;
               }
               
               
               
               inline CompressionState getCompressionState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask =  (1 << (2)) - 1;
   mask = static_cast<int>(mask << (0));
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (0));
   assertion(( tmp >= 0 &&  tmp <= 2));
   return (CompressionState) tmp;
               }
               
               
               
               inline void setCompressionState(const CompressionState& compressionState) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion((compressionState >= 0 && compressionState <= 2));
   int mask =  (1 << (2)) - 1;
   mask = static_cast<int>(mask << (0));
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 | static_cast<int>(compressionState) << (0));
               }
               
               
               
               inline int getBytesPerDoFInPreviousSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (2));
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (2));
   tmp = tmp + 1;
   assertion(( tmp >= 1 &&  tmp <= 7));
   return (int) tmp;
               }
               
               
               
               inline void setBytesPerDoFInPreviousSolution(const int& bytesPerDoFInPreviousSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion((bytesPerDoFInPreviousSolution >= 1 && bytesPerDoFInPreviousSolution <= 7));
   int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (2));
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 | (static_cast<int>(bytesPerDoFInPreviousSolution) - 1) << (2));
               }
               
               
               
               inline int getBytesPerDoFInSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (5));
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (5));
   tmp = tmp + 1;
   assertion(( tmp >= 1 &&  tmp <= 7));
   return (int) tmp;
               }
               
               
               
               inline void setBytesPerDoFInSolution(const int& bytesPerDoFInSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion((bytesPerDoFInSolution >= 1 && bytesPerDoFInSolution <= 7));
   int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (5));
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 | (static_cast<int>(bytesPerDoFInSolution) - 1) << (5));
               }
               
               
               
               inline int getBytesPerDoFInExtrapolatedSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (8));
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (8));
   tmp = tmp + 1;
   assertion(( tmp >= 1 &&  tmp <= 7));
   return (int) tmp;
               }
               
               
               
               inline void setBytesPerDoFInExtrapolatedSolution(const int& bytesPerDoFInExtrapolatedSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion((bytesPerDoFInExtrapolatedSolution >= 1 && bytesPerDoFInExtrapolatedSolution <= 7));
   int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (8));
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 | (static_cast<int>(bytesPerDoFInExtrapolatedSolution) - 1) << (8));
               }
               
               
               
               inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._level;
               }
               
               
               
               inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._level = level;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<DIMENSIONS,double> getOffset() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._offset;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setOffset(const tarch::la::Vector<DIMENSIONS,double>& offset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._offset = (offset);
               }
               
               
               
               inline double getOffset(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS);
                  return _persistentRecords._offset[elementIndex];
                  
               }
               
               
               
               inline void setOffset(int elementIndex, const double& offset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS);
                  _persistentRecords._offset[elementIndex]= offset;
                  
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<DIMENSIONS,double> getSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._size;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setSize(const tarch::la::Vector<DIMENSIONS,double>& size) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._size = (size);
               }
               
               
               
               inline double getSize(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS);
                  return _persistentRecords._size[elementIndex];
                  
               }
               
               
               
               inline void setSize(int elementIndex, const double& size) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS);
                  _persistentRecords._size[elementIndex]= size;
                  
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline std::bitset<DIMENSIONS_TIMES_TWO> getNeighbourMergePerformed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._neighbourMergePerformed;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setNeighbourMergePerformed(const std::bitset<DIMENSIONS_TIMES_TWO>& neighbourMergePerformed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._neighbourMergePerformed = (neighbourMergePerformed);
               }
               
               
               
               inline bool getNeighbourMergePerformed(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                  return _persistentRecords._neighbourMergePerformed[elementIndex];
                  
               }
               
               
               
               inline void setNeighbourMergePerformed(int elementIndex, const bool& neighbourMergePerformed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                  _persistentRecords._neighbourMergePerformed[elementIndex]= neighbourMergePerformed;
                  
               }
               
               
               
               inline void flipNeighbourMergePerformed(int elementIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                  _persistentRecords._neighbourMergePerformed.flip(elementIndex);
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline std::bitset<DIMENSIONS_TIMES_TWO> getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._isInside;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setIsInside(const std::bitset<DIMENSIONS_TIMES_TWO>& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._isInside = (isInside);
               }
               
               
               
               inline bool getIsInside(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                  return _persistentRecords._isInside[elementIndex];
                  
               }
               
               
               
               inline void setIsInside(int elementIndex, const bool& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                  _persistentRecords._isInside[elementIndex]= isInside;
                  
               }
               
               
               
               inline void flipIsInside(int elementIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                  _persistentRecords._isInside.flip(elementIndex);
               }
               
               
               
               inline Type getType() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._type;
               }
               
               
               
               inline void setType(const Type& type) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._type = type;
               }
               
               
               
               inline int getParentIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._parentIndex;
               }
               
               
               
               inline void setParentIndex(const int& parentIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._parentIndex = parentIndex;
               }
               
               
               
               inline RefinementEvent getRefinementEvent() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._refinementEvent;
               }
               
               
               
               inline void setRefinementEvent(const RefinementEvent& refinementEvent) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._refinementEvent = refinementEvent;
               }
               
               
               /**
                * Generated
                */
               static std::string toString(const CompressionState& param);
               
               /**
                * Generated
                */
               static std::string getCompressionStateMapping();
               
               /**
                * Generated
                */
               static std::string toString(const Type& param);
               
               /**
                * Generated
                */
               static std::string getTypeMapping();
               
               /**
                * Generated
                */
               static std::string toString(const RefinementEvent& param);
               
               /**
                * Generated
                */
               static std::string getRefinementEventMapping();
               
               /**
                * Generated
                */
               std::string toString() const;
               
               /**
                * Generated
                */
               void toString(std::ostream& out) const;
               
               
               PersistentRecords getPersistentRecords() const;
               /**
                * Generated
                */
               FiniteVolumesCellDescription convert() const;
               
               
            #ifdef Parallel
               protected:
                  static tarch::logging::Log _log;
                  
               public:
                  
                  /**
                   * Global that represents the mpi datatype.
                   * There are two variants: Datatype identifies only those attributes marked with
                   * parallelise. FullDatatype instead identifies the whole record with all fields.
                   */
                  static MPI_Datatype Datatype;
                  static MPI_Datatype FullDatatype;
                  
                  /**
                   * Initializes the data type for the mpi operations. Has to be called
                   * before the very first send or receive operation is called.
                   */
                  static void initDatatype();
                  
                  static void shutdownDatatype();
                  
                  /**
                   * @param communicateSleep -1 Data exchange through blocking mpi
                   * @param communicateSleep  0 Data exchange through non-blocking mpi, i.e. pending messages are received via polling until MPI_Test succeeds
                   * @param communicateSleep >0 Same as 0 but in addition, each unsuccessful MPI_Test is follows by an usleep
                   */
                  void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, int communicateSleep);
                  
                  void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, int communicateSleep);
                  
                  static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                  
                  #endif
         
      };
      
      #ifdef PackedRecords
      #pragma pack (pop)
      #endif
      
      
      
   
#endif

#endif

