Packed-Type: short int;

class exahype::dastgen::State {    
  /**
   * Consult Peano guidebook Section 6.3.2
   */
  persistent parallelise int maxRefinementLevelAllowed;
  
  /**
   * This enum is used to specify
   * which data we want to merge in the
   * Merging mapping.
   * 
   * Note that we define sending and merging here 
   * as operation between individual cells not
   * just between subdomains belong to different threads
   * or MPI processes.
   * 
   * For example, an ADER-DG solver might send
   * data to its face data arrays or a neighbour rank's heap in one
   * iteration.
   * In the next iteration, the data is picked up and merrged by its 
   * direct neighbour which might be a local cell
   * or a cell belonging to another rank.
   */
  enum MergeMode {
    /**
     * Do not merge anything.
     */
    MergeNothing,

    /**
     * This is one of the modes
     * for the standard time stepping
     * algorithm.
     *
     * This mode should be set before
     * performing the correction phase.
     *
     * Time step data exchange is
     * a reduction-broadcast operation.
     */
    BroadcastAndMergeTimeStepData,
    
    /**
     * This is one of the modes
     * for the standard time stepping
     * algorithm.
     *
     * This mode should be set before
     * performing the prediction phase.
     *
     * In principal, face data exchange is
     * direct neighbour communication.
     * However if we consider adaptive meshes,
     * face data exchange also includes
     * master-worker and worker-master
     * communication.
     */
    MergeFaceData,
    
    /**
     * Drop face data.
     */
    DropFaceData,
    
    /**
     * This is the default mode if you use
     * fused time stepping.
     *
     * In this case, we exchange
     * time step as well as
     * face data.
     */
    BroadcastAndMergeTimeStepDataAndMergeFaceData,
    
    /**
     * Broadcast time step data from the 
     * master to the workers 
     * and drop face data.
     */
    BroadcastAndMergeTimeStepDataAndDropFaceData
  };

  parallelise persistent MergeMode mergeMode;

  /**
   * This enum is used to specify
   * which data we want to send in the
   * Sending mapping.
   * 
   * Note that we define sending and merging here 
   * as operations between individual cells not
   * just between subdomains belong to different threads
   * or MPI processes. 
   * 
   * For example, an ADER-DG solver might send
   * data to its face data arrays or a neighbour rank's heap in
   * one iteration.
   * In the next iteration, the data is picked up and merrged by its 
   * direct neighbour which might be a local cell
   * or a cell belonging to another rank.
   */
  enum SendMode {
    /**
     * Do not send anything.
     */
    SendNothing,

    /**
     * This is one of the modes
     * for the standard time stepping
     * algorithm.
     *
     * This mode should be set before
     * performing the correction phase.
     *
     * Time step data exchange is
     * a reduction-broadcast operation.
     */
    ReduceAndMergeTimeStepData,
    /**
     * This is one of the modes
     * for the standard time stepping
     * algorithm.
     *
     * This mode should be set before
     * performing the prediction phase.
     *
     * In principal, face data exchange is
     * direct neighbour communication.
     * However if we consider adaptive meshes,
     * face data exchange also includes
     * as master-worker and worker-master
     * communication.
     */
    SendFaceData,
    /**
     * Choose this mode if you use
     * fused time stepping.
     *
     * In this case, we exchange
     * time step as well as
     * face data. We thus overlap the
     * reduction-broadcast with
     * the master-worker and
     * worker-master communication.
     */
    ReduceAndMergeTimeStepDataAndSendFaceData
  };
  parallelise persistent SendMode sendMode;
  
  /**
   * This flag influences the behaviour of the Predictor,
   * TimeStepSizeComputation, SolutionUpdate, Merging and
   * Sending mappings by selecting only the solvers that
   * are active in the given algorithmic section.
   */
  enum AlgorithmSection {
   	/*
   	 * The runner is currently 
   	 * performing a normal ADER-DG time step.
   	 */
   	TimeStepping,
   	
   	/**
   	 * The runner performs mesh refinement according to the
   	 * user's refinement criterion or the regular limiter status
   	 * flagging, i.e. the LimiterStatus updates caused by a 
   	 * regularly moving limiter domain or since
   	 * a global recomputation is necessary.
   	 */
   	MeshRefinement,
   	
   	/**
   	 * In this section, the runner overlaps the 
   	 * operations that must be performed after the
   	 * mesh refinement with operations that
   	 * must be performed for a local or global
   	 * recomputation. 
   	 */
   	MeshRefinementOrLocalOrGlobalRecomputation,
   	
    /**
     * This marks an end point of the side branch.
     * It is sure here that no solver performs
     * a global recomputation or has performed mesh refinement
     * 
     * Triggers a send for all registered solvers.
     */
    LocalRecomputationAllSend,
    
    /**
     * In this section, the  runner
     * triggers operations for solvers that
     * have performed mesh refinement or
     * which perform a global recomputation.
     */
    MeshRefinementOrGlobalRecomputation,
   	
    /**
     * This marks an end point of the side branch.
     * It is sure here that no solver performs
     * a global recomputation.
     * 
     * Triggers a send for all registered solvers.
     */
    MeshRefinementAllSend,
    
   	/**
   	 * In this section, solvers finalise their 
   	 * global recomputation by doing a complete
   	 * ADERDGTimeStep. Furthermore
   	 * since this is the last adapter call
   	 * in the side branch, all solvers are ask
   	 * to send out data.
   	 */
   	GlobalRecomputationAllSend,
   	
   	/**
   	 * In this section, all solver have to drop
   	 * their messages. Then, the ADER-DG solvers which
   	 * have violated the CFL condition with their
   	 * estimated time step size are required
   	 * to reurn the prediction.
   	 * Finally, all solvers send out again their
   	 * face data.
   	 */
   	PredictionRerunAllSend
  };
  parallelise persistent AlgorithmSection _algorithmSection;
};
